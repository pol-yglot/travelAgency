#robots.txt : 웹사이트에서 크롤링하며 정보를 수집하는 검색엔진 크롤러(또는 검색 로봇)가 액세스 하거나 정보수집을 해도 되는 페이지가 무엇인지, 해서는 안 되는 페이지가 무엇인지 알려주는 역할을 하는 .txt (텍스트) 파일입니다
#User-agent: robots.txt 에서 지정하는 크롤링 규칙이 적용되어야 할 크롤러를 지정합니다.
#Allow: 크롤링을 허용할 경로입니다 (/ 부터의 상대 경로).
#Disallow: 크롤링을 제한할 경로입니다 (/ 부터의 상대 경로).
#Sitemap: 사이트맵이 위치한 경로의 전체 URL입니다 (https:// 부터 /sitemap.xml 까지의 전체 절대경로 URL).



User-agent: *
Disallow: /forbidden/